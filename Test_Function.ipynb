{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate tfrecord file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "from data import generate_standard_dataset\n",
    "import numpy as np\n",
    "\n",
    "normal_ls = generate_standard_dataset('/home/taivu/workspace/NudityDetection/Dataset/train/normal_1', 224, 224)\n",
    "nudity_ls = generate_standard_dataset('/home/taivu/workspace/NudityDetection/Dataset/train/nude_1', 224, 224)\n",
    "\n",
    "labels = np.zeros(4000, dtype = np.uint)\n",
    "\n",
    "dataset = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess, coord)\n",
    "    for idx in range(4000):\n",
    "        if idx %2 == 0:\n",
    "            img = normal_ls.eval()\n",
    "        else:\n",
    "            img = nudity_ls.eval()\n",
    "            labels[idx] = 1\n",
    "        dataset.append(img)\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads, stop_grace_period_secs = 5)\n",
    "    sess.close()\n",
    "\n",
    "dataset = np.array(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "from data import generate_standard_dataset\n",
    "import numpy as np\n",
    "\n",
    "normal_ls = generate_standard_dataset('/home/taivu/workspace/NudityDetection/Dataset/validation/normal', 224, 224)\n",
    "nudity_ls = generate_standard_dataset('/home/taivu/workspace/NudityDetection/Dataset/validation/nude', 224, 224)\n",
    "\n",
    "val_labels = np.zeros(400, dtype = np.uint)\n",
    "\n",
    "val_dataset = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess, coord)\n",
    "    for idx in range(400):\n",
    "        if idx %2 == 0:\n",
    "            img = normal_ls.eval()\n",
    "        else:\n",
    "            img = nudity_ls.eval()\n",
    "            val_labels[idx] = 1\n",
    "        val_dataset.append(img)\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads, stop_grace_period_secs = 5)\n",
    "    sess.close()\n",
    "\n",
    "val_dataset = np.array(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "from data import generate_standard_dataset\n",
    "import numpy as np\n",
    "\n",
    "normal_ls = generate_standard_dataset('/home/taivu/workspace/NudityDetection/Dataset/test/normal_jpeg', 224, 224, '*jpeg')\n",
    "nudity_ls = generate_standard_dataset('/home/taivu/workspace/NudityDetection/Dataset/test/nude_jpeg', 224, 224, '*.jpeg')\n",
    "\n",
    "test_labels = np.zeros(981, dtype=np.uint)\n",
    "\n",
    "test_dataset = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess, coord)\n",
    "    for idx in range(981):\n",
    "        if idx < 681:\n",
    "            img = normal_ls.eval()\n",
    "        else:\n",
    "            img = nudity_ls.eval()\n",
    "            test_labels[idx] = 1\n",
    "            \n",
    "        test_dataset.append(img)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads, stop_grace_period_secs=5)\n",
    "    sess.close()\n",
    "\n",
    "test_dataset = np.array(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Reading Data Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from data import generate_standard_dataset\n",
    "from data import _int64_feature\n",
    "from data import _bytes_feature\n",
    "\n",
    "normal_ls, _ = generate_standard_dataset('/media/taivu/Data/Collecting_Dataset/total_normal_images', 224, 224, '*.jpeg')\n",
    "nudity_ls, ls_name = generate_standard_dataset('/media/taivu/Data/Collecting_Dataset/total_nudity_images', 224, 224)\n",
    "\n",
    "dataset_dir = '/media/taivu/Data/Project/Train_Result/Dataset'\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    threads = tf.train.start_queue_runners(sess, coord)\n",
    "    for batch in range(3, 8):\n",
    "        # Create a tfrecord file\n",
    "        file_name = '4000x224x224_batch_' + str(batch) + '.tfrecords'\n",
    "        file_path = os.path.join(dataset_dir, file_name)\n",
    "        writer = tf.python_io.TFRecordWriter(file_path)\n",
    "        \n",
    "        for idx in range(4000):\n",
    "            img=None\n",
    "            lb=0\n",
    "            if idx % 2 == 0:\n",
    "                img = normal_ls.eval()\n",
    "                lb = 0\n",
    "            else:\n",
    "                img = nudity_ls.eval()\n",
    "                lb = 1\n",
    "            if img is not None:\n",
    "                raw_img = img.tostring()\n",
    "                example = tf.train.Example(features=tf.train.Features(\n",
    "                    feature={'label': _int64_feature(int(lb)),\n",
    "                            'image_raw': _bytes_feature(raw_img)}))\n",
    "                writer.write(example.SerializeToString())\n",
    "        \n",
    "        writer.close()\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/taivu/Data/Collecting_Dataset/total_nudity_images/11-1_cpp_740.jpg\n"
     ]
    }
   ],
   "source": [
    "print ls_name[9672]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save preprocessed images into a tfrecords-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data import generate_tfrecords\n",
    "\n",
    "generate_tfrecords('/home/taivu/workspace/Pycharm_Nudity_Detection/Dataset', dataset,\n",
    "                  labels, '4000x224x224_batch_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data import generate_tfrecords\n",
    "\n",
    "generate_tfrecords('/home/taivu/workspace/Pycharm_Nudity_Detection/Dataset', val_dataset,\n",
    "                  val_labels, '400x224x224_eval_batch_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data import generate_tfrecords\n",
    "\n",
    "generate_tfrecords('/home/taivu/workspace/Pycharm_Nudity_Detection/Dataset', test_dataset,\n",
    "                  test_labels, '981x244x244_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Training - Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processsing...\n",
      "Reading processed images, and labels\n",
      "Initialziing....\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data import generate_tfrecords\n",
    "from data import extract_features\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#coord = tf.train.Coordinator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    img_batch, lb_batch = extract_features(sess, tfrecord_path='/home/taivu/workspace/Pycharm_Nudity_Detection/Dataset/4000x299x299_train_set.tfrecords', num_samples=4000)\n",
    "\n",
    "img_batch_np = np.array(img_batch)\n",
    "lb_batch_np = np.array(lb_batch)\n",
    "\n",
    "generate_tfrecords('/home/taivu/workspace/Pycharm_Nudity_Detection/Dataset', img_batch_np, lb_batch_np,\n",
    "                  'transfer_learning_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Validation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processsing...\n",
      "Reading processed images, and labels\n",
      "Initialziing....\n"
     ]
    }
   ],
   "source": [
    "from data import generate_tfrecords\n",
    "from data import extract_features\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    val_img_batch, val_lb_batch = extract_features(sess, tfrecord_path='/home/taivu/workspace/Pycharm_Nudity_Detection/Dataset/1156x299x299_val_set.tfrecords', num_samples=1156)\n",
    "\n",
    "val_img_batch_np = np.array(val_img_batch)\n",
    "val_lb_batch_np = np.array(val_lb_batch)\n",
    "\n",
    "generate_tfrecords('/home/taivu/workspace/Pycharm_Nudity_Detection/Dataset', val_img_batch_np, val_lb_batch_np,\n",
    "                  'transfer_learning_val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using ResNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This temp code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vng_model import _variable_with_weight_decay\n",
    "from vng_model import _initialize_variable\n",
    "from vng_model import inference_resnet\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim.python.slim.nets import resnet_v1\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    is_training = True\n",
    "    x = tf.placeholder(tf.float32, (None, 224, 224, 3), name='input_features')\n",
    "    y_ = tf.placeholder(tf.float32, (None,), name='labels')\n",
    "\n",
    "    with slim.arg_scope(resnet_v1.resnet_arg_scope(is_training)):\n",
    "        net, end_points = resnet_v1.resnet_v1_50(x)\n",
    "    \n",
    "    net = tf.squeeze(net, [1, 2])\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        ckpt = tf.train.get_checkpoint_state('/home/taivu/workspace/Pycharm_Nudity_Detection/pretrain_weight')\n",
    "    \n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "        else:\n",
    "            print('No checkpoint file found')\n",
    "    \n",
    "    # Additional layers\n",
    "    with tf.variable_scope('Additional_scope'):\n",
    "        with tf.variable_scope('FC_1') as scope:\n",
    "            weights = _variable_with_weight_decay('weights',\n",
    "                                                 [2048, 1024],\n",
    "                                                 0.04, 0.004)\n",
    "\n",
    "            biases = _initialize_variable('biases',\n",
    "                                         [1024],\n",
    "                                         tf.constant_initializer(0.1))\n",
    "\n",
    "            activate_1 = tf.nn.relu(tf.matmul(net, weights) + biases, name = scope.name)\n",
    "\n",
    "        with tf.variable_scope('Softmax') as scope:\n",
    "            weights = _variable_with_weight_decay('weights',\n",
    "                                                 [1024, 2],\n",
    "                                                 stddev=1/1024.0, wd=0.0)\n",
    "\n",
    "            biases = _initialize_variable('biases',\n",
    "                                         [2],\n",
    "                                         tf.constant_initializer(0.0))\n",
    "\n",
    "            softmax_classifier = tf.add(tf.matmul(activate_1, weights), biases, name=scope.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var_list = g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "\n",
    "#print len(var_list)\n",
    "for idx, var in zip(range(len(var_list)), var_list):\n",
    "    print idx, var.name\n",
    "    # print var.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import data as dt\n",
    "import vng_model as md\n",
    "import os\n",
    "\n",
    "train_dir = '/media/taivu/Data/Project/Train_Result/Dataset'\n",
    "batch_ls = []\n",
    "for batch in range(2, 8):\n",
    "    name_batch = '4000x224x224_batch_' + str(batch) + '.tfrecords'\n",
    "    train_batch = os.path.join(train_dir, name_batch)\n",
    "    batch_ls.append(train_batch)\n",
    "\n",
    "val_path = os.path.join(train_dir, '4000x224x224_batch_1.tfrecords')\n",
    "\n",
    "with tf.Graph().as_default() as g1:\n",
    "    # ------------------------- BUILD THE GRAPH OF MODEL ---------------------------- #\n",
    "    x = tf.placeholder(tf.float32, (None, 224, 224, 3), name='input_features')\n",
    "    y_ = tf.placeholder(tf.int32, (None,), name='labels')\n",
    "\n",
    "    val_x = tf.placeholder(tf.float32, (None, 224, 224, 3), name='val_input_features')\n",
    "\n",
    "    val_y = tf.placeholder(tf.int32, (None,), name='val_labels')\n",
    "\n",
    "    tr_samples, tr_labels = dt.input_data(batch_ls, 40)\n",
    "\n",
    "    val_samples, val_labels = dt.input_data([val_path], 100, False)\n",
    "\n",
    "    logit = md.inference_resnet(x)\n",
    "\n",
    "    val_logit = md.inference_resnet(val_x, False, reuse=True)\n",
    "\n",
    "    # Define variables to output the predict of model and to evaluate one\n",
    "    resnet_var_ls = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='resnet_v1_50')\n",
    "    resnet_weight_ls = []\n",
    "    for idx in range(0, 159, 3):\n",
    "        resnet_weight_ls.append(resnet_var_ls[idx])\n",
    "\n",
    "    loss = md.loss(logit, y_, resnet_weight_ls)\n",
    "\n",
    "    v_loss = md.loss(val_logit, val_y, resnet_weight_ls)\n",
    "\n",
    "    hat_y = tf.arg_max(val_logit, 1, name='predict_label')\n",
    "\n",
    "    correct_pre = tf.equal(tf.cast(hat_y, tf.int32), val_y)\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pre, tf.float32))\n",
    "    # ------------------------------------- END -------------------------------------- #\n",
    "\n",
    "    # -------------------------------Optimizing process ------------------------------ #\n",
    "    resnet_var_ls = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='resnet_v1_50')\n",
    "\n",
    "    add_var_ls = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='additional_layers')\n",
    "\n",
    "    opt_1 = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "    opt_2 = tf.train.GradientDescentOptimizer(5*0.01)\n",
    "\n",
    "    # Freeze the weights of from first to third blocks\n",
    "    grads = tf.gradients(loss, resnet_var_ls[153:] + add_var_ls)\n",
    "\n",
    "    # Do gradient descent only on a particular weight set\n",
    "    num_opt_resnet_layers = len(resnet_var_ls[153:])\n",
    "\n",
    "    grads_1 = grads[:num_opt_resnet_layers]  # Do gradient for Resnet's layers\n",
    "\n",
    "    grads_2 = grads[num_opt_resnet_layers:]  # Do gradient for Additional layers\n",
    "\n",
    "    train_opt_1 = opt_1.apply_gradients(zip(grads_1, resnet_var_ls[153:]))\n",
    "\n",
    "    train_opt_2 = opt_2.apply_gradients(zip(grads_2, add_var_ls))\n",
    "\n",
    "    train_opt = tf.group(train_opt_1, train_opt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 resnet_v1_50/conv1/weights:0\n",
      "1 resnet_v1_50/conv1/BatchNorm/beta:0\n",
      "2 resnet_v1_50/conv1/BatchNorm/gamma:0\n",
      "3 resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/weights:0\n",
      "4 resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/beta:0\n",
      "5 resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma:0\n",
      "6 resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/weights:0\n",
      "7 resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "8 resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "9 resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/weights:0\n",
      "10 resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "11 resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "12 resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/weights:0\n",
      "13 resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "14 resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "15 resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/weights:0\n",
      "16 resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "17 resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "18 resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/weights:0\n",
      "19 resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "20 resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "21 resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/weights:0\n",
      "22 resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "23 resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "24 resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/weights:0\n",
      "25 resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "26 resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "27 resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/weights:0\n",
      "28 resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "29 resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "30 resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/weights:0\n",
      "31 resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "32 resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "33 resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/weights:0\n",
      "34 resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/beta:0\n",
      "35 resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma:0\n",
      "36 resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/weights:0\n",
      "37 resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "38 resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "39 resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/weights:0\n",
      "40 resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "41 resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "42 resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/weights:0\n",
      "43 resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "44 resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "45 resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/weights:0\n",
      "46 resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "47 resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "48 resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/weights:0\n",
      "49 resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "50 resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "51 resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/weights:0\n",
      "52 resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "53 resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "54 resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/weights:0\n",
      "55 resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "56 resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "57 resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/weights:0\n",
      "58 resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "59 resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "60 resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/weights:0\n",
      "61 resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "62 resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "63 resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/weights:0\n",
      "64 resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "65 resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "66 resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/weights:0\n",
      "67 resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "68 resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "69 resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/weights:0\n",
      "70 resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "71 resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "72 resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/weights:0\n",
      "73 resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/beta:0\n",
      "74 resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma:0\n",
      "75 resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/weights:0\n",
      "76 resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "77 resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "78 resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/weights:0\n",
      "79 resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "80 resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "81 resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/weights:0\n",
      "82 resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "83 resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "84 resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/weights:0\n",
      "85 resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "86 resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "87 resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/weights:0\n",
      "88 resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "89 resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "90 resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/weights:0\n",
      "91 resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "92 resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "93 resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/weights:0\n",
      "94 resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "95 resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "96 resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/weights:0\n",
      "97 resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "98 resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "99 resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/weights:0\n",
      "100 resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "101 resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "102 resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/weights:0\n",
      "103 resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "104 resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "105 resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/weights:0\n",
      "106 resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "107 resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "108 resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/weights:0\n",
      "109 resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "110 resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "111 resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/weights:0\n",
      "112 resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "113 resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "114 resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/weights:0\n",
      "115 resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "116 resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "117 resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/weights:0\n",
      "118 resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "119 resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "120 resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/weights:0\n",
      "121 resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "122 resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "123 resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/weights:0\n",
      "124 resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "125 resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "126 resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/weights:0\n",
      "127 resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "128 resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "129 resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/weights:0\n",
      "130 resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/beta:0\n",
      "131 resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma:0\n",
      "132 resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/weights:0\n",
      "133 resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "134 resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "135 resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/weights:0\n",
      "136 resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "137 resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "138 resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/weights:0\n",
      "139 resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "140 resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "141 resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/weights:0\n",
      "142 resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "143 resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "144 resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/weights:0\n",
      "145 resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "146 resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "147 resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/weights:0\n",
      "148 resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "149 resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "150 resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/weights:0\n",
      "151 resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/BatchNorm/beta:0\n",
      "152 resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/BatchNorm/gamma:0\n",
      "153 resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/weights:0\n",
      "154 resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/BatchNorm/beta:0\n",
      "155 resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/BatchNorm/gamma:0\n",
      "156 resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/weights:0\n",
      "157 resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/BatchNorm/beta:0\n",
      "158 resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/BatchNorm/gamma:0\n",
      "159 additional_layers/softmax/weights:0\n",
      "160 additional_layers/softmax/biases:0\n"
     ]
    }
   ],
   "source": [
    "var_ls = g1.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "for idx, var in zip(range(len(var_ls)), var_ls):\n",
    "    print idx, var.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from vng_model import _variable_with_weight_decay\n",
    "from vng_model import _initialize_variable\n",
    "from vng_model import inference_resnet\n",
    "from vng_model import loss\n",
    "from data import input_data\n",
    "\n",
    "train_path = os.path.join('/home/taivu/workspace/Pycharm_Nudity_Detection/Dataset',\n",
    "                          '4000x224x224_batch_2.tfrecords')\n",
    "\n",
    "checkpoint_dir = '/home/taivu/workspace/Pycharm_Nudity_Detection/pretrain_weight'\n",
    "with tf.Graph().as_default():\n",
    "    is_training = True\n",
    "    x = tf.placeholder(tf.float32, (None, 224, 224, 3), name='input_features')\n",
    "    y_ = tf.placeholder(tf.float32, (None,), name='labels')\n",
    "    \n",
    "    samples, labels = input_data(train_path, 32)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        logit = inference_resnet(sess, x, checkpoint_dir, True)\n",
    "        \n",
    "    loss = loss(logit, y_)\n",
    "    \n",
    "    train_step = tf.train.RMSPropOptimizer(2e-3).minimize(loss)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for idx in range(10000):\n",
    "            tr_x, tr_y = sess.run([samples, labels])\n",
    "            \n",
    "            _, loss_value = sess.run([train_step, loss], feed_dict={x:tr_x, y_:tr_y})\n",
    "            \n",
    "            print('Step %d: %0.2f'%(idx, loss_value))\n",
    "            \n",
    "        coord.request_stop()\n",
    "        coord.join(threads, stop_grace_period_secs=120)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Step 0: 5.88 (7.3 samples/sec; 13.649 secs/batch)\n",
      "Validation accuracy: 0.50\n",
      "Step 10: 5.79 (7.5 samples/sec; 13.290 secs/batch)\n",
      "Validation accuracy: 0.53\n",
      "Step 20: 6.15 (7.6 samples/sec; 13.238 secs/batch)\n",
      "Validation accuracy: 0.50\n",
      "Step 30: 5.84 (7.6 samples/sec; 13.116 secs/batch)\n",
      "Validation accuracy: 0.53\n",
      "Step 40: 5.94 (7.5 samples/sec; 13.409 secs/batch)\n",
      "Validation accuracy: 0.50\n",
      "Step 50: 5.72 (7.2 samples/sec; 13.965 secs/batch)\n",
      "Validation accuracy: 0.55\n",
      "Step 60: 5.74 (7.5 samples/sec; 13.250 secs/batch)\n",
      "Validation accuracy: 0.50\n",
      "Step 70: 5.78 (7.5 samples/sec; 13.314 secs/batch)\n",
      "Validation accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "from train_model import train_resnet\n",
    "\n",
    "train_resnet(True,val_dataset, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from data import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "train_path = os.path.join('/home/taivu/workspace/Pycharm_Nudity_Detection/Dataset',\n",
    "                          '4000x224x224_batch_2.tfrecords')\n",
    "\n",
    "samples, labels = input_data(train_path, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the previously trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "v1 = tf.Variable(1., name='v1')\n",
    "v2 = tf.Variable(2., name='v2')\n",
    "\n",
    "a = tf.add(v1, v2)\n",
    "\n",
    "all_saver = tf.train.Saver()\n",
    "\n",
    "v2_saver = tf.train.Saver({\"v2\": v2})\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    all_saver.save(sess, '/home/taivu/workspace/NudityDetection/Test/'+'all_val.ckpt')\n",
    "    \n",
    "    v2_saver.save(sess, '/home/taivu/workspace/NudityDetection/Test/'+'v2.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1:0\n",
      "v2:0\n",
      "v1:0\n",
      "v2:0\n",
      "v1:0\n",
      "v2:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "new_saver = tf.train.import_meta_graph(\n",
    "    '/home/taivu/workspace/NudityDetection/Test/all_val.ckpt.meta')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state('/home/taivu/workspace/NudityDetection/Test')\n",
    "    \n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        new_saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print('No checkpoint file found!')\n",
    "        \n",
    "    all_vars = tf.trainable_variables()\n",
    "\n",
    "for v in all_vars:\n",
    "    print(v.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0.551020386815\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim.python.slim.nets import resnet_v1\n",
    "from vng_model import _variable_with_weight_decay\n",
    "from vng_model import _initialize_variable\n",
    "from vng_model import inference_resnet\n",
    "import data as dt\n",
    "slim = tf.contrib.slim\n",
    "import os\n",
    "\n",
    "#new_saver = tf.train.import_meta_graph(\n",
    "#    '/home/taivu/workspace/Pycharm_Nudity_Detection/checkpoint_model/model.ckpt-18960.meta')\n",
    "\n",
    "test_path = os.path.join( '/home/taivu/workspace/Pycharm_Nudity_Detection/Dataset',\n",
    "                        '981x244x244_test.tfrecords')\n",
    "\n",
    "\n",
    "\n",
    "############## Load test set ###################\n",
    "\n",
    "\n",
    "#coord = tf.train.Coordinator()\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "#    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "#    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#    val_data, val_lb = sess.run([val_samples, val_labels])\n",
    "    \n",
    "#    coord.request_stop()\n",
    "#    coord.join(threads)\n",
    "#    sess.close()\n",
    "\n",
    "################################################\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(tf.float32, (None, 224, 224, 3), \n",
    "                   name='input_features')\n",
    "    \n",
    "    y_ = tf.placeholder(tf.int32, (None,), name='labels')\n",
    "    \n",
    "    test_samples, test_labels = dt.input_data([test_path], 98, False)\n",
    "    \n",
    "    logit = inference_resnet(x)\n",
    "    \n",
    "    hat_y = tf.arg_max(logit, 1, name='predict_label')\n",
    "    \n",
    "    correct_pre = tf.equal(tf.cast(hat_y, tf.int32), y_)\n",
    "    \n",
    "    acc = tf.reduce_mean(tf.cast(correct_pre, tf.float32))\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state('/home/taivu/workspace/Pycharm_Nudity_Detection/checkpoint_model')\n",
    "        \n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print('No checkpoint file found!')\n",
    "        \n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        \n",
    "        for idx in range(10):\n",
    "            print idx\n",
    "            test_data, test_lb = sess.run([test_samples, test_labels])\n",
    "            accuracy_test = sess.run(acc, feed_dict={x:test_data, y_: test_lb})\n",
    "            if idx == 0:\n",
    "                mean_acc = accuracy_test\n",
    "            else:\n",
    "                mean_acc = 1.0/(idx + 1)*(accuracy_test + idx*mean_acc)\n",
    "        \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "            \n",
    "    print mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim.python.slim.nets import resnet_v1\n",
    "from vng_model import _variable_with_weight_decay\n",
    "from vng_model import _initialize_variable\n",
    "import data as dt\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "checkpoint_dir = '/home/taivu/workspace/Pycharm_Nudity_Detection/pretrain_weight'\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(tf.float32, (None, 224, 224, 3), name='input_features')\n",
    "    y_ = tf.placeholder(tf.int32, (None,), name='labels')\n",
    "    \n",
    "    with slim.arg_scope(resnet_v1.resnet_arg_scope(True)):\n",
    "        net, end_points = resnet_v1.resnet_v1_50(x)\n",
    "\n",
    "    net = tf.squeeze(net, [1, 2])\n",
    "    \n",
    "    with tf.variable_scope('additional_layers'):\n",
    "        with tf.variable_scope('FC_1') as scope:\n",
    "            weights = _variable_with_weight_decay('weights',\n",
    "                                                  [2048, 1024],\n",
    "                                                  0.04, 0.004)\n",
    "\n",
    "            biases = _initialize_variable('biases',\n",
    "                                          [1024],\n",
    "                                          tf.constant_initializer(0.1))\n",
    "\n",
    "            activate_1 = tf.nn.relu(tf.matmul(net, weights) + biases, name=scope.name)\n",
    "\n",
    "        with tf.variable_scope('softmax') as scope:\n",
    "            weights = _variable_with_weight_decay('weights',\n",
    "                                                  [1024, 2],\n",
    "                                                  stddev=1 / 1024.0, wd=0.0)\n",
    "\n",
    "            biases = _initialize_variable('biases',\n",
    "                                          [2],\n",
    "                                          tf.constant_initializer(0.0))\n",
    "\n",
    "            softmax_classifier = tf.add(tf.matmul(activate_1, weights), biases, name=scope.name)\n",
    "    \n",
    "    \n",
    "    saver = tf.train.Saver(var_list=\n",
    "                           tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='resnet_v1_50'))\n",
    "    \n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print('Successful!')\n",
    "        else:\n",
    "            print('Checkpoint not found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test reading many tfrecords files\n",
    " We create two tfrecord files to test reading a batch of tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from data import _int64_feature\n",
    "\n",
    "A_features = np.array(range(100))\n",
    "B_features = np.array(range(100,200))\n",
    "\n",
    "file_name_a = os.path.join('/home/taivu/Dropbox/Pycharm_Nudity_Detection', 'feature_a.tfrecords')\n",
    "file_name_b = os.path.join('/home/taivu/Dropbox/Pycharm_Nudity_Detection', 'feature_b.tfrecords')\n",
    "\n",
    "writer = tf.python_io.TFRecordWriter(file_name_a)\n",
    "\n",
    "for idx in range(100):\n",
    "    example = tf.train.Example(features=tf.train.Features(\n",
    "        feature={'raw': _int64_feature(int(A_features[idx]))}\n",
    "    ))\n",
    "    writer.write(example.SerializeToString())\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.python_io.TFRecordWriter(file_name_b)\n",
    "\n",
    "for idx in range(100):\n",
    "    example = tf.train.Example(features=tf.train.Features(\n",
    "        feature={'raw': _int64_feature(int(B_features[idx]))}\n",
    "    ))\n",
    "    writer.write(example.SerializeToString())\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def read_and_decode(filename_queue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={'raw':tf.FixedLenFeature([], tf.int64)})\n",
    "    \n",
    "    obj = tf.cast(features['raw'], tf.int32)\n",
    "    \n",
    "    return obj\n",
    "\n",
    "def input_data(data_dir, batch_size):\n",
    "    filename_queue = tf.train.string_input_producer(data_dir)\n",
    "    \n",
    "    obj = read_and_decode(filename_queue)\n",
    "    \n",
    "    batch_obj = tf.train.batch(\n",
    "        [obj], batch_size=batch_size, capacity=10+3*batch_size)\n",
    "    \n",
    "    return batch_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 101 102 103 104]\n",
      "[105 106 107 108 109]\n",
      "[110 111 112 113 114]\n",
      "[115 116 117 118 119]\n",
      "[120 121 122 123 124]\n",
      "[125 126 127 128 129]\n",
      "[130 131 132 133 134]\n",
      "[135 136 137 138 139]\n",
      "[140 141 142 143 144]\n",
      "[145 146 147 148 149]\n",
      "[150 151 152 153 154]\n",
      "[155 156 157 158 159]\n",
      "[160 161 162 163 164]\n",
      "[165 166 167 168 169]\n",
      "[170 171 172 173 174]\n",
      "[175 176 177 178 179]\n",
      "[180 181 182 183 184]\n",
      "[185 186 187 188 189]\n",
      "[190 191 192 193 194]\n",
      "[195 196 197 198 199]\n",
      "[0 1 2 3 4]\n",
      "[5 6 7 8 9]\n",
      "[10 11 12 13 14]\n",
      "[15 16 17 18 19]\n",
      "[20 21 22 23 24]\n",
      "[25 26 27 28 29]\n",
      "[30 31 32 33 34]\n",
      "[35 36 37 38 39]\n",
      "[40 41 42 43 44]\n",
      "[45 46 47 48 49]\n",
      "[50 51 52 53 54]\n",
      "[55 56 57 58 59]\n",
      "[60 61 62 63 64]\n",
      "[65 66 67 68 69]\n",
      "[70 71 72 73 74]\n",
      "[75 76 77 78 79]\n",
      "[80 81 82 83 84]\n",
      "[85 86 87 88 89]\n",
      "[90 91 92 93 94]\n",
      "[95 96 97 98 99]\n",
      "[100 101 102 103 104]\n",
      "[105 106 107 108 109]\n",
      "[110 111 112 113 114]\n",
      "[115 116 117 118 119]\n",
      "[120 121 122 123 124]\n",
      "[125 126 127 128 129]\n",
      "[130 131 132 133 134]\n",
      "[135 136 137 138 139]\n",
      "[140 141 142 143 144]\n",
      "[145 146 147 148 149]\n",
      "[150 151 152 153 154]\n",
      "[155 156 157 158 159]\n",
      "[160 161 162 163 164]\n",
      "[165 166 167 168 169]\n",
      "[170 171 172 173 174]\n",
      "[175 176 177 178 179]\n",
      "[180 181 182 183 184]\n",
      "[185 186 187 188 189]\n",
      "[190 191 192 193 194]\n",
      "[195 196 197 198 199]\n",
      "[0 1 2 3 4]\n",
      "[5 6 7 8 9]\n",
      "[10 11 12 13 14]\n",
      "[15 16 17 18 19]\n",
      "[20 21 22 23 24]\n",
      "[25 26 27 28 29]\n",
      "[30 31 32 33 34]\n",
      "[35 36 37 38 39]\n",
      "[40 41 42 43 44]\n",
      "[45 46 47 48 49]\n",
      "[50 51 52 53 54]\n",
      "[55 56 57 58 59]\n",
      "[60 61 62 63 64]\n",
      "[65 66 67 68 69]\n",
      "[70 71 72 73 74]\n",
      "[75 76 77 78 79]\n",
      "[80 81 82 83 84]\n",
      "[85 86 87 88 89]\n",
      "[90 91 92 93 94]\n",
      "[95 96 97 98 99]\n",
      "[0 1 2 3 4]\n",
      "[5 6 7 8 9]\n",
      "[10 11 12 13 14]\n",
      "[15 16 17 18 19]\n",
      "[20 21 22 23 24]\n",
      "[25 26 27 28 29]\n",
      "[30 31 32 33 34]\n",
      "[35 36 37 38 39]\n",
      "[40 41 42 43 44]\n",
      "[45 46 47 48 49]\n",
      "[50 51 52 53 54]\n",
      "[55 56 57 58 59]\n",
      "[60 61 62 63 64]\n",
      "[65 66 67 68 69]\n",
      "[70 71 72 73 74]\n",
      "[75 76 77 78 79]\n",
      "[80 81 82 83 84]\n",
      "[85 86 87 88 89]\n",
      "[90 91 92 93 94]\n",
      "[95 96 97 98 99]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "batch_1 = os.path.join('/home/taivu/Dropbox/Pycharm_Nudity_Detection', 'feature_a.tfrecords')\n",
    "batch_2 = os.path.join('/home/taivu/Dropbox/Pycharm_Nudity_Detection', 'feature_b.tfrecords')\n",
    "\n",
    "bat_ob = input_data([batch_1, batch_2], 5)\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    for i in range(100):\n",
    "        sample = sess.run(bat_ob)\n",
    "        print sample\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print int(math.ceil(3.0/2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
